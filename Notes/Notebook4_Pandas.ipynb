{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a DataFrame.\n",
    "DataFrames are essentially multidimensional arrays with attached row and column labels.\n",
    "Series and DataFrame objects, builds on the NumPy array structure and provides efficient access to these sorts of \"data munging\" tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas Series Object\n",
    "A Pandas Series is a one-dimensional array of indexed data (similar to dictionary). It can be created from a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.50, 0.75, 1.00])\n",
    "\n",
    "print(data.values) # gives values of the series\n",
    "print(data.index) # gives indexes of the series, same with print(data.keys())\n",
    "print(list(data.items()))\n",
    "print()\n",
    "\n",
    "print(data[1:3]) # we can use slicing\n",
    "print()\n",
    "\n",
    "print(data[(data > 0.3) & (data < 0.8)]) # we can use masking\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.50, 0.75, 1.00], index=[\"a\", \"b\", \"c\", \"d\"], name=\"data\") # we can adjust indexing type of a series object\n",
    "print(data[\"b\":\"c\"]) # same with print(data[1:3]), it will include \"c\"\n",
    "print()\n",
    "\n",
    "data = pd.Series({\"a\":1, \"b\":2}) # we can use a dictionary to create a series\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas Dataframe Object\n",
    "\n",
    "If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame is an analog of a two-dimensional array with both flexible row indices and flexible column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "\n",
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "\n",
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "print(states)\n",
    "print(states.index) # gives the rows of the dataframe\n",
    "print(states.columns) # gives the columns of the dataframe\n",
    "print(states.area) # same with states[\"area\"]\n",
    "print(states[\"population\"]) # same with states.population\n",
    "\n",
    "states[\"density\"] = states[\"population\"] / states[\"area\"] # we can add a new column by doing an operation\n",
    "\n",
    "print(states)\n",
    "\n",
    "# Even if some keys in the dictionary are missing, Pandas will fill them in with NaN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access the data of a dataframe, we can use this method\n",
    "points_table = {'Team':['MI', 'CSK', 'Devils', 'MI', 'CSK',\n",
    "   'RCB', 'CSK', 'CSK', 'KKR', 'KKR', 'KKR', 'RCB'],\n",
    "   'Rank' :[1, 2, 2, 3, 3, 4, 1, 1, 2, 4, 1, 2],\n",
    "   'Year' :[2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
    "   'Point':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
    "df = pd.DataFrame(points_table)\n",
    "\n",
    "# df.head() gives the first n rows of the data, default=5\n",
    "# df.tail() gives the last n rows of the data, default=5\n",
    "# df.sample() gives random n rows of the data, default=5\n",
    "\n",
    "print(df.columns, end=\"\\n\\n\") # this will return all columns of the df as an Index object\n",
    "\n",
    "# So we can access the columns of the data\n",
    "print(df[0:2], end=\"\\n\\n\") # accesing rows by using indexing\n",
    "\n",
    "# To choose certain columns and rows of a dataframe\n",
    "teams = df[[\"Team\", \"Rank\"]].iloc[[2, 4, 6]] # for a dataframe: first index is column, for iloc and loc: first index is row\n",
    "print(teams, end=\"\\n\\n\")\n",
    "\n",
    "# We can narrow down the df by using masking\n",
    "best = df.loc[df['Rank'] == 3] # we can use comparison operators(|(or), &(and))\n",
    "print(best, end=\"\\n\\n\")\n",
    "\n",
    "teams = df[\"Team\"].tolist() # we can also manipulate the data by turning into a list\n",
    "teams[0] = \"RMA\"\n",
    "df[\"Team\"] = teams\n",
    "print(df.head(), end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to read a csv file : dataframe = pd.read_csv(\"data.csv\")\n",
    "\n",
    "dataframe.isna().sum()\n",
    "\n",
    "We use the drop(), dropna(), fill(), fillna() functions for deleting and filling the specific column or to delete the multiple columns at the same time.\n",
    "\n",
    "DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors=’raise’)\\\n",
    "labels: single label or list (index or name of the columns to drop)\\\n",
    "axis: {0 or ‘index’, 1 or ‘columns’}, it’s default value is 0\\\n",
    "columns: It is the same as the label or we can say that it is an alternative to specify the names of the attributes (colums=labels)\\\n",
    "level: If there are multiple indexes present in the DataFrame then we will pass the level\\\n",
    "inplace: If false then return a copy. Otherwise do operation inplace and return none"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "newDF = df.copy().head(20) # to return a copy not view\n",
    "\n",
    "M = newDF[\"Calories\"].mean()\n",
    "newDF[\"Calories\"].fillna(M, inplace=True)\n",
    "\n",
    "# inplace: If false then return a copy. Otherwise do operation inplace and return none.\n",
    "\n",
    "newDF[\"Pulse\"].fillna(0, inplace=True)\n",
    "\n",
    "newDF = newDF.dropna() # default: inplace=False, it returns a copy.\n",
    "\n",
    "print(newDF)\n",
    "print()\n",
    "last = newDF.loc[newDF[\"Maxpulse\"] > 135]\n",
    "print(last)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas DataFrame operates much like a structured array, and can be created directly from one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])\n",
    "\n",
    "data = pd.DataFrame(A)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas Index Object\n",
    "Immutable array, duplication allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA = pd.Index([1, 3, 5, 7, 9])\n",
    "indB = pd.Index([2, 3, 5, 7, 11])\n",
    "indC = indA.intersection(indB) # union() for\n",
    "print(indC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexers: loc, iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
    "\n",
    "# loc attribute allows indexing and slicing that always references the explicit index(starting from 1)\n",
    "print(data.loc[1:3])\n",
    "print()\n",
    "\n",
    "# iloc attribute allows indexing and slicing that always references the implicit Python-style index(starting from 0)\n",
    "print(data.iloc[1:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "None is a Python singleton object that is often used for missing data in Python code.\n",
    "Because it is a Python object, None cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type 'object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.array([1, None, 3, 4]) # we can create an array with a None object\n",
    "\n",
    "\n",
    "for dtype in ['object', 'int']:\n",
    "    np.arange(1E6, dtype=dtype).sum() # 1E6 = 1*10^6.0\n",
    "# This dtype=object means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects.\n",
    "# While this kind of object array is useful for some purposes, any operations on the data will be done at the Python level\n",
    "\n",
    "# The other missing data representation, NaN (acronym for Not a Number)\n",
    "\n",
    "vals2 = np.array([1, np.nan, 3, 4]) # dtype is float64 so it runs faster\n",
    "\n",
    "# vals2.sum(), vals2.min(), vals2.max() (they will return nan)\n",
    "# to deal with nan values we should use np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2) (ignore nan values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = pd.Series([2, 4, 6], index=[0, 1, 2])\n",
    "B = pd.Series([1, 3, 5], index=[1, 2, 3])\n",
    "C = A + B\n",
    "print(C, end=\"\\n\\n\")\n",
    "\n",
    "C = A.add(B, fill_value=0) # we filled NaN values with 0\n",
    "print(C)\n",
    "\n",
    "# a number + NaN = NaN\n",
    "# a number *  np.nan = nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating on Null Values\n",
    "As we have seen, Pandas treats None and NaN as essentially interchangeable for indicating missing or null values.\\\n",
    "Python uses the keyword None to define null objects and variables.\\\n",
    "While None does serve some of the same purposes as null in other languages but as the null in Python, None is not defined to be 0 or any other value.\\\n",
    "In Python, None is an object. To check if something is None, we should use the is / is not identity operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isnull(): Generate a boolean mask indicating missing values\n",
    "# notnull(): Opposite of isnull()\n",
    "# dropna(): Return a filtered version of the data\n",
    "# fillna(): Return a copy of the data with missing values filled or imputed\n",
    "\n",
    "# Filling null values\n",
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "print(data.fillna(0)) # returns a copy of series with filled nan values\n",
    "\n",
    "# forward-fill (returns a copy of series with forward-filled nan values)\n",
    "data.fillna(method='ffill')\n",
    "# back-fill (returns a copy of series with back-filled nan values)\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Indexing: Pandas MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose you would like to track data about states from two different years. This is the bad way for doing it\n",
    "index = [('California', 2000), ('California', 2010),\n",
    "         ('New York', 2000), ('New York', 2010),\n",
    "         ('Texas', 2000), ('Texas', 2010)]\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "pop = pd.Series(populations, index=index)\n",
    "print(pop)\n",
    "print()\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(index) # we can use MultiIndex to track the data\n",
    "pop = pop.reindex(index)\n",
    "print(pop)\n",
    "print()\n",
    "\n",
    "# You might notice something else here: we could easily have stored the same data using a simple DataFrame with index and column labels.\n",
    "# In fact, Pandas is built with this equivalence in mind. The unstack() method will quickly convert a multiply indexed Series into a conventionally indexed DataFrame.\n",
    "pop_df = pop.unstack()\n",
    "\n",
    "# Naturally, the stack() method provides the opposite operation\n",
    "pop_df.stack()\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# In a DataFrame, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well.\n",
    "\n",
    "# hierarchical indices and columns\n",
    "index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],\n",
    "                                   names=['year', 'visit'])\n",
    "columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']],\n",
    "                                     names=['subject', 'type'])\n",
    "\n",
    "# mock some data\n",
    "data = np.round(np.random.randn(4, 6), 1)\n",
    "data[:, ::2] *= 10\n",
    "data += 37\n",
    "\n",
    "# create the DataFrame\n",
    "health_data = pd.DataFrame(data, index=index, columns=columns)\n",
    "print(health_data)\n",
    "\n",
    "# Many of the MultiIndex slicing operations will fail if the index is not sorted.\n",
    "# So we use: data = data.sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets\n",
    "Pandas has a function, pd.concat(), which has a similar syntax to np.concatenate that can be used for simple concatenations of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat() can be used for a simple concatenation of Series or DataFrame objects\n",
    "# pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "#           keys=None, levels=None, names=None, verify_integrity=False,\n",
    "#           copy=True)\n",
    "\n",
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])\n",
    "\n",
    "# If you'd like to simply verify that the indices in the result of pd.concat() do not overlap, you can specify the verify_integrity flag.\n",
    "# Sometimes the index itself does not matter, and you would prefer it to simply be ignored. This option can be specified using the ignore_index flag.\n",
    "\n",
    "# Appending the data: ser1.append(ser2)\n",
    "# Keep in mind that unlike the append() and extend() methods of Python lists, the append() method in Pandas does not modify the original object–instead it creates a new object with the combined data.\n",
    "# It also is not a very efficient method, because it involves creation of a new index and data buffer.\n",
    "# Thus, if you plan to do multiple append operations, it is generally better to build a list of DataFrames and pass them all at once to the concat() function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)\n",
    "\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "print(display('df1', 'df2'))\n",
    "print()\n",
    "\n",
    "# To combine this information into a single DataFrame, we can use the pd.merge() function.\n",
    "df3 = pd.merge(df1, df2)\n",
    "print(df3)\n",
    "# The pd.merge() function recognizes that each DataFrame has an \"employee\" column, and automatically joins using this column as a key.\n",
    "print(\"-----------------------------------------------\")\n",
    "# Most simply, you can explicitly specify the name of the key column using the on keyword\n",
    "print(display('df1', 'df2', \"pd.merge(df1, df2, on='employee')\"))\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# For convenience, DataFrames implement the join() method, which performs a merge that defaults to joining on indices\n",
    "# df1.join(df2)\n",
    "\n",
    "df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n",
    "                    'food': ['fish', 'beans', 'bread']},\n",
    "                   columns=['name', 'food'])\n",
    "df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n",
    "                    'drink': ['wine', 'beer']},\n",
    "                   columns=['name', 'drink'])\n",
    "print(display('df6', 'df7', 'pd.merge(df6, df7)'))\n",
    "# Here we have merged two datasets that have only a single \"name\" entry in common: Mary.\n",
    "# By default, the result contains the intersection of the two sets of inputs; this is what is known as an inner join.\n",
    "# We can specify this explicitly using the how keyword, which defaults to \"inner\"\n",
    "\n",
    "# Other options for the how keyword are 'outer', 'left', and 'right'.\n",
    "# An outer join returns a join over the union of the input columns, and fills in all missing values with NAs\n",
    "print(display('df6', 'df7', \"pd.merge(df6, df7, how='outer')\"))\n",
    "\n",
    "# The left join and right join return joins over the left entries and right entries, respectively.\n",
    "print(display('df6', 'df7', \"pd.merge(df6, df7, how='left')\"))\n",
    "\n",
    "# Finally, you may end up in a case where your two input DataFrames have conflicting column names.\n",
    "# the merge function automatically appends a suffix _x or _y to make the output columns unique.\n",
    "# If these defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword\n",
    "\n",
    "# Examples on https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html\n",
    "\n",
    "# head() function returns the first n row of the data, default 5\n",
    "# tail() function returns the last n row of the data, default 5\n",
    "# sample() function returns randomly rows of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6)}, columns=['key', 'data'])\n",
    "print(df)\n",
    "print()\n",
    "print(df.groupby('key')) # It gives an DataFrameGroupBy object which means that common aggregates can be implemented very efficiently\n",
    "# To produce a result, we can apply an aggregate to this DataFrameGroupBy object\n",
    "print(df.groupby('key').sum())\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# GroupBy objects have aggregate(), filter(), transform(), and apply() methods that efficiently implement a variety of useful operations before combining the grouped data.\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0, 10, 6)},\n",
    "                   columns = ['key', 'data1', 'data2'])\n",
    "\n",
    "print(df)\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# Aggregation: Birleştirmek\n",
    "# aggregate() method can take a string, a function, or a list thereof, and compute all the aggregates at once.\n",
    "df1 = df.groupby('key').aggregate({'data1': 'min',\n",
    "                                   'data2': 'max'})\n",
    "print(df1)\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# Filtering\n",
    "# A filtering operation allows you to drop data based on the group properties.\n",
    "def filter_func(x):\n",
    "    return x['data2'].std() > 4\n",
    "\n",
    "print(display('df', \"df.groupby('key').std()\", \"df.groupby('key').filter(filter_func)\"))\n",
    "# The filter function should return a Boolean value specifying whether the group passes the filtering.\n",
    "# Here because group A does not have a standard deviation greater than 4, it is dropped from the result.\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# Transformation\n",
    "# While aggregation must return a reduced version of the data, transformation can return some transformed version of the full data to recombine.\n",
    "df2 = df.groupby('key').transform(lambda x: x - x.mean())\n",
    "print(df2)\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# The apply() method\n",
    "# The apply() method lets you apply an arbitrary function to the group results.\n",
    "# The function should take a DataFrame, and return either a Pandas object (e.g., DataFrame, Series) or a scalar; the combine operation will be tailored to the type of output returned.\n",
    "def norm_by_data2(x):\n",
    "    # x is a DataFrame of group values\n",
    "    x['data1'] /= x['data2'].sum()\n",
    "    return x\n",
    "\n",
    "print(display('df', \"df.groupby('key').apply(norm_by_data2)\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "A pivot table is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the database of passengers on the Titanic, available through the Seaborn library\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Pivot Table Syntax\n",
    "print(titanic.pivot_table('survived', index='sex', columns='class'))\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# Multi-level pivot tables\n",
    "age = pd.cut(titanic['age'], [0, 18, 80])\n",
    "print(titanic.pivot_table('survived', ['sex', age], 'class'))\n",
    "\n",
    "# DataFrame.pivot_table(data, values=None, index=None, columns=None,\n",
    "#                      aggfunc='mean', fill_value=None, margins=False,\n",
    "#                      dropna=True, margins_name='All')\n",
    "# margins gives All column which gives sum of values at the same row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', sep=\",\") # we can use sep attribute to seperate the data by a specific seperator\n",
    "# skiprows=[]\n",
    "# \n",
    "print(df.head()) # on jupyter, we can read this data by: !more data.csv\n",
    "\n",
    "# df.to_csv('out.csv') to create a new file and read the data in it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.csv')\n",
    "\n",
    "reader = csv.reader(f) # this will read the file as a csvreader object\n",
    "\n",
    "with open('data.csv') as f:\n",
    "    lines = list(csv.reader(f))\n",
    "print(lines[0:5])\n",
    "\n",
    "# JSON: https://www.w3schools.com/js/js_json_intro.asp more popular\n",
    "# XML: https://www.w3schools.com/xml/xml_whatis.asp\n",
    "# HTML: https://www.w3schools.com/html/html_intro.asp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\",\n",
    " \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 38,\n",
    "               \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n",
    "}\n",
    "\"\"\"\n",
    "result = json.loads(obj)\n",
    "print(result)\n",
    "\n",
    "# tables = pd.read_html('examples/fdic_failed_bank_list.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Data Formats\n",
    "One of the easiest ways to store data (serialization) efficiently in binary format is using pickle built-in library on Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv('data.csv') # by using pandas, we can turn the data into a pickle\n",
    "frame.to_pickle('examples/frame_pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Microsoft Excel Files\n",
    "To read data from a excel file\\\n",
    "xlsx = pd.ExcelFile('examples/ex1.xlsx')\\\n",
    "pd.read_excel(xlsx, 'Sheet1')\\\n",
    "\\\n",
    "To create and write data in a excel file\\\n",
    "writer = pd.ExcelWriter('examples/ex2.xlsx')\\\n",
    "frame.to_excel(writer, 'Sheet1')\\\n",
    "writer.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Web APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "resp = requests.get(url)\n",
    "print(resp)\n",
    "\n",
    "data = resp.json()\n",
    "print(data[0:5])\n",
    "\n",
    "issues = pd.DataFrame(data, columns=['number', 'title',\n",
    "                                     'labels', 'state'])\n",
    "print(issues.head())\n",
    "\n",
    "# https://pandas.pydata.org/docs/user_guide/io.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e791fc644de8a2a45a233fd18641ae2f3b7ed840a58b84ec7e10871a0d7e49c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
